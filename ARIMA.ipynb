{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARIMA",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1K-SuFRyFXs0-Cj47Ly3J16UZjiNRCrhD",
      "authorship_tag": "ABX9TyMgvcaf0/Wirf+pWrEiiSyv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sukhi02/Smart_Fasal_in/blob/master/ARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4VPv6IteENC",
        "outputId": "5b8bdfb4-b9a3-45a1-e2d2-c32639ca4289"
      },
      "source": [
        "!pip install pyramid\r\n",
        "!pip install pyramid.arima \r\n",
        "import pandas as pd\r\n",
        "from pandas import read_csv\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.metrics import mean_absolute_error\r\n",
        "from math import sqrt\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "\r\n",
        "#!pip install pyramid\r\n",
        "#!pip install pyramid.arima \r\n",
        "from pyramid.arima import auto_arima\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from statsmodels.tsa.arima_model import ARIMA\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "#\r\n",
        "\r\n",
        "import pyramid.arima as pa\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "file_name = \"/content/drive/MyDrive/SmartFasal/dataset.csv\"\r\n",
        "\r\n",
        "raw_dataset  = read_csv(file_name, header=0, index_col=0)\r\n",
        "print(raw_dataset.head(10))\r\n",
        "print()\r\n",
        "print(raw_dataset.describe())\r\n",
        "print()\r\n",
        "print(\"Libraries and Dataset imported\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyramid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/5a/6f934f9cf154aaf74469c2665029b473bb553ac0e1c9aa25f6d4d7891333/pyramid-1.10.5-py2.py3-none-any.whl (326kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 20.0MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 12.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 61kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 81kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 184kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 194kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 204kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 215kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 225kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 235kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 245kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 256kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 266kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 276kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 286kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 296kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 307kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 317kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 327kB 8.2MB/s \n",
            "\u001b[?25hCollecting plaster\n",
            "  Downloading https://files.pythonhosted.org/packages/61/29/3ac8a5d03b2d9e6b876385066676472ba4acf93677acfc7360b035503d49/plaster-1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pyramid) (51.1.1)\n",
            "Collecting zope.interface>=3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 10.1MB/s \n",
            "\u001b[?25hCollecting zope.deprecation>=3.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f9/26/b935bbf9d27e898b87d80e7873a0200cebf239253d0afe7a59f82fe90fff/zope.deprecation-4.4.0-py2.py3-none-any.whl\n",
            "Collecting hupper>=1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/48/7f/06ace28143b2cb3a4b14c9d9e5165741d2d133ef331b616acf47ab5c3517/hupper-1.10.2-py2.py3-none-any.whl\n",
            "Collecting plaster-pastedeploy\n",
            "  Downloading https://files.pythonhosted.org/packages/11/c4/0470056ea324c7a420c22647be512dec1b5e32b1b6e77e27c61838d2811c/plaster_pastedeploy-0.7-py2.py3-none-any.whl\n",
            "Collecting translationstring>=0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/98/36187601a15e3d37e9bfcf0e0e1055532b39d044353b06861c3a519737a9/translationstring-1.4-py2.py3-none-any.whl\n",
            "Collecting webob>=1.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/3c/de37900faff3c95c7d55dd557aa71bd77477950048983dcd4b53f96fde40/WebOb-1.8.6-py2.py3-none-any.whl (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 18.7MB/s \n",
            "\u001b[?25hCollecting venusian>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/43/92/3d522a710867168ee422a0ffbd712c425ece937aaeec4381497a59e24faf/venusian-3.0.0-py3-none-any.whl\n",
            "Collecting PasteDeploy>=2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/0b/d47ea894587f3155f8c4520aa74d57c856189d0bbe27e831881d655a3386/PasteDeploy-2.1.1-py2.py3-none-any.whl\n",
            "Installing collected packages: plaster, zope.interface, zope.deprecation, hupper, PasteDeploy, plaster-pastedeploy, translationstring, webob, venusian, pyramid\n",
            "Successfully installed PasteDeploy-2.1.1 hupper-1.10.2 plaster-1.0 plaster-pastedeploy-0.7 pyramid-1.10.5 translationstring-1.4 venusian-3.0.0 webob-1.8.6 zope.deprecation-4.4.0 zope.interface-5.2.0\n",
            "Collecting pyramid.arima\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/84/57422f2a6ade3161c586545e38b518ba1b7ab30ee4a4acc29110c0aba2bc/pyramid_arima-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (597kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.23 in /usr/local/lib/python3.6/dist-packages (from pyramid.arima) (0.29.21)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from pyramid.arima) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from pyramid.arima) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pyramid.arima) (1.1.5)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from pyramid.arima) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from pyramid.arima) (0.22.2.post1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pyramid.arima) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pyramid.arima) (2.8.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->pyramid.arima) (0.5.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17->pyramid.arima) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19->pyramid.arima) (1.15.0)\n",
            "Installing collected packages: pyramid.arima\n",
            "Successfully installed pyramid.arima\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pyramid/__init__.py:68: UserWarning: \n",
            "    The 'pyramid' package will be migrating to a new namespace beginning in \n",
            "    version 1.0.0: 'pmdarima'. This is due to a package name collision with the\n",
            "    Pyramid web framework. For more information, see Issue #34:\n",
            "    \n",
            "        https://github.com/tgsmith61591/pyramid/issues/34\n",
            "        \n",
            "    The package will subsequently be installable via the name 'pmdarima'; the\n",
            "    only functional change to the user will be the import name. All imports\n",
            "    from 'pyramid' will change to 'pmdarima'.\n",
            "    \n",
            "  \"\"\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                         SM1          SM2  ...         PRSR         LMNS\n",
            "Time IST                                   ...                          \n",
            "3/28/2020 14:00  2650.190000  2982.130000  ...  98909.46667  3372.000000\n",
            "3/28/2020 14:15  2673.858571  2963.604286  ...  98909.50571  3419.142857\n",
            "3/28/2020 14:30  2717.523333  2961.506667  ...  98907.45000  3400.166667\n",
            "3/28/2020 14:45  2739.771667  2970.316667  ...  98906.00667  3494.166667\n",
            "3/28/2020 15:00  2784.721429  2978.735714  ...  98901.55714  4713.000000\n",
            "3/28/2020 15:15  2865.441667  2991.040000  ...  98895.59167  5195.166667\n",
            "3/28/2020 15:30  2938.440000  2997.015000  ...  98890.66833  5365.166667\n",
            "3/28/2020 15:45  2996.648571  2999.161429  ...  98888.62286  4681.571429\n",
            "3/28/2020 16:00  3036.606667  3012.040000  ...  98880.24667  4938.500000\n",
            "3/28/2020 16:15  3081.070000  2999.161429  ...  98867.47286  5145.857143\n",
            "\n",
            "[10 rows x 7 columns]\n",
            "\n",
            "               SM1          SM2  ...          PRSR         LMNS\n",
            "count  7188.000000  7188.000000  ...   7188.000000  7188.000000\n",
            "mean   2921.550939  5787.331986  ...  97810.023289  1003.594009\n",
            "std    1871.435821  1555.744483  ...    409.388409  1587.574609\n",
            "min     302.483333  2626.980000  ...  96716.423330     0.000000\n",
            "25%    1459.748250  4504.500000  ...  97499.423750     0.000000\n",
            "50%    2535.201750  6250.000000  ...  97834.562835   197.000000\n",
            "75%    4166.660000  7142.850000  ...  98126.788750  1474.000000\n",
            "max    7317.416667  8196.720000  ...  98909.505710  9150.333333\n",
            "\n",
            "[8 rows x 7 columns]\n",
            "\n",
            "Libraries and Dataset imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N5LnHe51nWb"
      },
      "source": [
        "Training and Testing subsets\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC0a5xau4jb1"
      },
      "source": [
        "Scaling of the dataset\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpV68XgwCTOB"
      },
      "source": [
        "111\r\n",
        "\r\n",
        "011\r\n",
        "\r\n",
        "011\r\n",
        "\r\n",
        "\r\n",
        "3 1 2\r\n",
        "\r\n",
        "110   3 2 3\r\n",
        "\r\n",
        "11 1 0  223\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw2BjEBLFrH_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNWX78XqfImK"
      },
      "source": [
        "Second ARIMA model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXgzsBVrfKtb"
      },
      "source": [
        "def second_ARIMA_model(train_data_10, test_data_10, p,d,q):\r\n",
        "  print()\r\n",
        "  print(\"The model will be trained on  p = \" + str(p) +  \", d = \" + str(d) + \"q =\" + str(q))\r\n",
        "  print()\r\n",
        "  print()\r\n",
        "  print(\"Model Building\")\r\n",
        "  print()\r\n",
        "  print()\r\n",
        "  \r\n",
        "  history = [x for x in train_data_10]\r\n",
        "  predictions = list()\r\n",
        "  for t in range(len(test_data_10)):\r\n",
        "\t  model_10 = ARIMA(history, order=(3,1,3), missing='drop')\r\n",
        "\t  model_10_fit = model_10.fit(disp=0)\r\n",
        "\t  output = model_10_fit.forecast()\r\n",
        "\t  yhat = output[0]\r\n",
        "\t  predictions.append(yhat)\r\n",
        "\t  obs = test_data_10[t]\r\n",
        "\t  history.append(obs)\r\n",
        "\t  print('10m ',t ,' predicted=%f, expected=%f' % (yhat, obs))  \r\n",
        "  testPredict_10  = predictions  \r\n",
        "  return testPredict_10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-7aeF5PJzLG"
      },
      "source": [
        "A moddel of Auto Arima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFLfsaQPJ3Et"
      },
      "source": [
        "def run_the_arima_model(train_data,d):\r\n",
        "  i = 0\r\n",
        "  for i in range(0,d):\r\n",
        "    print()\r\n",
        "    print(\"The differecing order is  \" + str(i) )\r\n",
        "    print()\r\n",
        "    print()\r\n",
        "        #!pip install pyramid\r\n",
        "    #!pip install pyramid.arima \r\n",
        "    #import_all_libraries()\r\n",
        "    from pyramid.arima import auto_arima\r\n",
        "    stepwise_model = auto_arima(train_data, start_p=0, start_q=0,\r\n",
        "                            max_p=10, max_q=10, d = i, max_d = 4, trace=True,\r\n",
        "                            error_action='ignore',  \r\n",
        "                            suppress_warnings=True, \r\n",
        "                            stepwise=True)\r\n",
        "    print(stepwise_model.aic())  \r\n",
        "    print(\"####### AIC plotted #######\")\r\n",
        "    print(\"Okay\")\r\n",
        "    print()\r\n",
        "    print()\r\n",
        "    print(\"************************\")\r\n",
        "    print()\r\n",
        "    print(\"Fitting of the model\")\r\n",
        "    print()\r\n",
        "    #stepwise_model.fit(train_data)\r\n",
        "    print(\"####### Model fitted #######\")\r\n",
        "    print(\"Okay\")\r\n",
        "    print()\r\n",
        "    #trained_data = stepwise_model.predict(n_periods=49)\r\n",
        "    #print(trained_data)\r\n",
        "    #return trained_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufbMVc_vJzvX"
      },
      "source": [
        "EValuation of the Trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTgXhBOkNrq6"
      },
      "source": [
        "def evaluate_the_trained_ARIMA_model(test_data_10 , testPredict_10):\r\n",
        "  #import_all_libraries()\r\n",
        "  from sklearn.metrics import r2_score\r\n",
        "  from sklearn.metrics import mean_squared_error\r\n",
        "  from sklearn.metrics import mean_absolute_error\r\n",
        "  from math import sqrt\r\n",
        "  #trainPredict_10 = sc.inverse_transform(Y_10)\r\n",
        "  error_10 = mean_squared_error(test_data_10 , testPredict_10)\r\n",
        "  print('Test MSE: %.3f' % error_10)\r\n",
        "\r\n",
        "  MAE_10= mean_absolute_error(test_data_10 , testPredict_10)\r\n",
        "  print('Test MAE: %.3f' % MAE_10)\r\n",
        "\r\n",
        "  rmse_10 = sqrt(mean_squared_error(test_data_10 , testPredict_10))\r\n",
        "  print('Test RMSE: %.3f' % rmse_10)\r\n",
        "\r\n",
        "  #MAPE_10= mean_absolute_percentage_error(test_data_10 , testPredict_10)\r\n",
        "  #print(\"\\n\\n \\tMean absolute percentage error  :\",MAPE_10)\r\n",
        "  #print(\".......Accuracy for 80 m  ...\", 100-MAPE_10)\r\n",
        "\r\n",
        "  from sklearn.metrics import r2_score\r\n",
        "  R2_10 = r2_score(test_data_10 , testPredict_10)\r\n",
        "  print(\"R2 score = \", ((R2_10)*100))\r\n",
        "\r\n",
        "  # plot\r\n",
        "  plt.plot(test_data_10, c = 'green', label = 'Actual')\r\n",
        "  plt.plot(testPredict_10, c = 'red', label = 'Predicted')\r\n",
        "  plt.xlabel('Time')\r\n",
        "  plt.ylabel('Moisture Level 10 cm')\r\n",
        "  plt.title('Forecasted moisture 10 cm')\r\n",
        "  plt.legend()\r\n",
        "  plt.savefig('Moisture Level at 10 ARIMA ')\r\n",
        "  plt.show()\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZONGMGKHSql"
      },
      "source": [
        "def  ARIMA_model(dataset, p,d,q):\r\n",
        "    # split into train and test sets\r\n",
        "  train_size = int(len(dataset) * 0.80)\r\n",
        "  test_size= len(dataset) - train_size\r\n",
        "  total_size = train_size+1\r\n",
        "  \r\n",
        "  #train_size = 7138\r\n",
        "  #test_size = 50\r\n",
        "  \r\n",
        "  from sklearn.preprocessing import MinMaxScaler\r\n",
        "  import numpy as np\r\n",
        "  \r\n",
        "  print(\"Training rows  -->>\" + str(train_size))\r\n",
        "  print()\r\n",
        "  print(\"Testing rows -->> \" + str(test_size))\r\n",
        "  print()\r\n",
        "  print()\r\n",
        "  train_data = dataset[:train_size]\r\n",
        "  print(\"T R A I N I N G    D A T A\")\r\n",
        "  print(train_data)\r\n",
        "  print()\r\n",
        "  print()\r\n",
        "  test_data  = dataset[train_size+1:]\r\n",
        "  print(\"T E S T I N G    D A T A\")\r\n",
        "  print(test_data)\r\n",
        "  print()\r\n",
        "  print()\r\n",
        "  print(\"####### data splitted #######\")\r\n",
        "  print(\"Okay\")\r\n",
        "  print()\r\n",
        "  print(\"Scaling of the dataset\")\r\n",
        "  sc = MinMaxScaler(feature_range = (0, 1))\r\n",
        "  train_scale_data = sc.fit_transform(train_data)\r\n",
        "  print(\"-----------------------\")\r\n",
        "  print(\"####### data scaled #######\")\r\n",
        "  print(\"Okay\")\r\n",
        "  print()\r\n",
        "  print()\r\n",
        "  print()\r\n",
        "  print(\"Running the ARIMA model\")\r\n",
        "  print()\r\n",
        "  trained_data = run_the_arima_model(train_data,d)\r\n",
        "\r\n",
        "  p = p\r\n",
        "  d = d\r\n",
        "  q = q\r\n",
        "\r\n",
        "  #trained_data = second_ARIMA_model(train_data, test_data, p, d , q)\r\n",
        "    \r\n",
        "  print()\r\n",
        "  print(\"Trained data\")\r\n",
        "  print(\"=========================\")\r\n",
        "  #trained_data = np.reshape(trained_data, (-1,1))\r\n",
        "  \r\n",
        "  print(\"Actual Data\")\r\n",
        "  print(test_data)\r\n",
        "  #trained_data_normal = sc.inverse_transform(trained_data)\r\n",
        "  #trained_data_normal = sc.inverse_transform(trained_data)\r\n",
        "  print()\r\n",
        "  print(\"_______________________\")\r\n",
        "  print(\"Testing Data\")\r\n",
        "  print(\"-----------------------\")\r\n",
        "  print(trained_data)\r\n",
        "  evaluate_the_trained_ARIMA_model(trained_data, test_data)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpzqiyO-Nq3O"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyUBGOFrNrTB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X53o46NE2JMV",
        "outputId": "994abb33-9efb-4de2-da2a-0955b5839a6a"
      },
      "source": [
        "dataset =               raw_dataset.iloc[:, 0:8].values\r\n",
        "dataset_10 =            raw_dataset.iloc[:, 0:1].values\r\n",
        "dataset_45 =            raw_dataset.iloc[:, 1:2].values\r\n",
        "dataset_80 =            raw_dataset.iloc[:, 2:3].values\r\n",
        "dataset_Temperature =   raw_dataset.iloc[:, 3:4].values\r\n",
        "dataset_Humidity =      raw_dataset.iloc[:, 4:5].values\r\n",
        "dataset_Pressure =      raw_dataset.iloc[:, 5:6].values\r\n",
        "dataset_Lum =           raw_dataset.iloc[:, 6:7].values\r\n",
        "print(\"dataset splitted\")\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset splitted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQDD2Bw7GUJP"
      },
      "source": [
        "Running the model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dobhc_Y5H-Dp",
        "outputId": "4ded34ed-7297-4e54-f3fa-6164fab2de34"
      },
      "source": [
        "ARIMA_model(dataset_Temperature, 1,4,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training rows  -->>5750\n",
            "\n",
            "Testing rows -->> 1438\n",
            "\n",
            "\n",
            "T R A I N I N G    D A T A\n",
            "[[22.89      ]\n",
            " [24.13142857]\n",
            " [25.36      ]\n",
            " ...\n",
            " [43.42666667]\n",
            " [40.28333333]\n",
            " [39.45      ]]\n",
            "\n",
            "\n",
            "T E S T I N G    D A T A\n",
            "[[43.28333333]\n",
            " [43.92      ]\n",
            " [44.25      ]\n",
            " ...\n",
            " [43.205     ]\n",
            " [44.8375    ]\n",
            " [45.27333333]]\n",
            "\n",
            "\n",
            "####### data splitted #######\n",
            "Okay\n",
            "\n",
            "Scaling of the dataset\n",
            "-----------------------\n",
            "####### data scaled #######\n",
            "Okay\n",
            "\n",
            "\n",
            "\n",
            "Running the ARIMA model\n",
            "\n",
            "\n",
            "The differecing order is  0\n",
            "\n",
            "\n",
            "Fit ARIMA: order=(0, 0, 0) seasonal_order=(0, 0, 0, 1); AIC=38542.946, BIC=38556.260, Fit time=0.105 seconds\n",
            "Fit ARIMA: order=(1, 0, 0) seasonal_order=(0, 0, 0, 1); AIC=14489.900, BIC=14509.871, Fit time=0.357 seconds\n",
            "Fit ARIMA: order=(0, 0, 1) seasonal_order=(0, 0, 0, 1); AIC=31057.213, BIC=31077.184, Fit time=1.337 seconds\n",
            "Fit ARIMA: order=(2, 0, 0) seasonal_order=(0, 0, 0, 1); AIC=12716.916, BIC=12743.543, Fit time=0.601 seconds\n",
            "Fit ARIMA: order=(2, 0, 1) seasonal_order=(0, 0, 0, 1); AIC=12662.340, BIC=12695.625, Fit time=3.224 seconds\n",
            "Fit ARIMA: order=(3, 0, 2) seasonal_order=(0, 0, 0, 1); AIC=12664.714, BIC=12711.313, Fit time=7.901 seconds\n",
            "Fit ARIMA: order=(1, 0, 1) seasonal_order=(0, 0, 0, 1); AIC=12828.969, BIC=12855.597, Fit time=3.079 seconds\n",
            "Fit ARIMA: order=(3, 0, 1) seasonal_order=(0, 0, 0, 1); AIC=12646.681, BIC=12686.623, Fit time=5.772 seconds\n",
            "Fit ARIMA: order=(3, 0, 0) seasonal_order=(0, 0, 0, 1); AIC=12685.605, BIC=12718.890, Fit time=1.194 seconds\n",
            "Fit ARIMA: order=(4, 0, 2) seasonal_order=(0, 0, 0, 1); AIC=12634.854, BIC=12688.109, Fit time=12.405 seconds\n",
            "Fit ARIMA: order=(5, 0, 2) seasonal_order=(0, 0, 0, 1); AIC=12437.626, BIC=12497.539, Fit time=23.671 seconds\n",
            "Fit ARIMA: order=(5, 0, 1) seasonal_order=(0, 0, 0, 1); AIC=12433.608, BIC=12486.863, Fit time=15.179 seconds\n",
            "Fit ARIMA: order=(4, 0, 0) seasonal_order=(0, 0, 0, 1); AIC=12587.231, BIC=12627.173, Fit time=1.579 seconds\n",
            "Fit ARIMA: order=(6, 0, 2) seasonal_order=(0, 0, 0, 1); AIC=12436.520, BIC=12503.090, Fit time=21.493 seconds\n",
            "Fit ARIMA: order=(4, 0, 1) seasonal_order=(0, 0, 0, 1); AIC=12433.667, BIC=12480.266, Fit time=11.988 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}